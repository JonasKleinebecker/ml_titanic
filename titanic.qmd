---
title: "Titanic Data Analysis"
jupyter: python3
---

## Imports
```{python}
import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col, sum, mean
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, MinMaxScaler
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, DoubleType
```
```{python}
spark = SparkSession.builder.appName("titanic").getOrCreate()
spark.version
```

## Load Data
```{python}
spark_train_df = spark.read.csv("train.csv", header=True, inferSchema=True)
pandas_train_df = pd.read_csv("train.csv")
spark_test_df = spark.read.csv("test.csv", header=True, inferSchema=True)
pandas_test_df = pd.read_csv("test.csv")
```

## Print Data
```{python}
spark_train_df.printSchema()
pandas_train_df.info()

spark_train_df.sample(withReplacement=False, fraction=0.01).show()
pandas_train_df.sample(5)

spark_train_df.describe().show(5)
pandas_train_df.describe()

print(spark_train_df.columns)
print(pandas_train_df.columns)
```

## Clean Data
### Remove Columns
```{python}
print(pandas_train_df.isnull().sum())
null_counts = spark_train_df.select([sum(col(c).isNull().cast("int")).alias(c) for c in spark_train_df.columns]).show()

spark_train_df = spark_train_df.drop("PassengerId", "Name", "Ticket", "Cabin")
spark_train_df.show(5)
pandas_train_df.drop(columns=["PassengerId", "Name", "Ticket", "Cabin"], inplace=True)
print(pandas_train_df.head(5))
```

### Convert Sex to Binary
```{python}
spark_train_df = spark_train_df.withColumn("Sex", when(spark_train_df[ "Sex" ] == "male", 0).when(spark_train_df[ "Sex" ] == "female", 1))
# pandas_train_df["Sex"] = pandas_train_df["Sex"].map({"male": 0, "female": 1})
pandas_train_df["Sex"] = np.where(pandas_train_df["Sex"] == "male", 0, 1)
```

### Deal with Null Values in Age
```{python}
# Evaluate the bias that would be introduced by dropping Rows with null Age
pandas_train_nullAge_df = pandas_train_df[pandas_train_df["Age"].isnull()]
pandas_train_df.describe() - pandas_train_nullAge_df.describe()
# Similar operation not as easy in pyspark

# Dropping Null Age rows introduces significant bias towards higher class and lower parch, therefore imputing instead
pandas_train_df["Age"] = pandas_train_df["Age"].fillna(pandas_train_df["Age"].mean())
spark_train_age_mean = spark_train_df.agg({"Age": "mean"}).collect()[0][0]
spark_train_df = spark_train_df.fillna({"Age": spark_train_age_mean})
spark_train_df.show(5)
print(pandas_train_df.sample(5))
```

### Create Family Size Column
```{python}
spark_train_df = spark_train_df.withColumn("FamilySize", spark_train_df["SibSp"] + spark_train_df["Parch"] + 1)
pandas_train_df["FamilySize"] = pandas_train_df["SibSp"] + pandas_train_df["Parch"] + 1
pandas_train_df = pandas_train_df.drop(columns=["SibSp", "Parch"])
spark_train_df = spark_train_df.drop("SibSp", "Parch")
```

### One Hot Encode Embarked Column
```{python}
# Keeping Embarked is questionable but doing so for one hot encoding practise
pandas_train_df = pd.get_dummies(pandas_train_df, columns=["Embarked"])

spark_string_indexer = StringIndexer(inputCol="Embarked", outputCol="EmbarkedIndex")
spark_train_df = spark_string_indexer.fit(spark_train_df).transform(spark_train_df)

spark_one_hot_encoder = OneHotEncoder(inputCol="EmbarkedIndex", outputCol="EmbarkedVec")
spark_train_df = spark_one_hot_encoder.fit(spark_train_df).transform(spark_train_df)
spark_train_df = spark_train_df.drop("EmbarkedIndex", "Embarked")
spark_train_df.show(5)
print(pandas_train_df.head(5))
```

## Normalize Data
```{python}
columns_to_normalize = ["Pclass", "Age", "Fare", "FamilySize"]
pandas_train_df[columns_to_normalize] = (pandas_train_df[columns_to_normalize] - pandas_train_df[columns_to_normalize].min()) / (pandas_train_df[columns_to_normalize].max() -
  pandas_train_df[columns_to_normalize].min())

assembler = VectorAssembler(inputCols=columns_to_normalize, outputCol="features")
spark_df_assembled = assembler.transform(spark_train_df)

scaler = MinMaxScaler(inputCol="features", outputCol="features_scaled")
spark_df_scaled = scaler.fit(spark_df_assembled).transform(spark_df_assembled)

to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))

spark_train_df = spark_df_scaled.withColumn("scaled_array", to_array(spark_df_scaled["features_scaled"]))

for i, col in enumerate(columns_to_normalize):
  spark_train_df =  spark_train_df.withColumn(f"{col}_normalized", spark_train_df["scaled_array"].getItem(i))

spark_train_df = spark_train_df.drop("scaled_array", "features", "features_scaled")
spark_train_df = spark_train_df.drop(*columns_to_normalize)
spark_train_df.show(5)
```


